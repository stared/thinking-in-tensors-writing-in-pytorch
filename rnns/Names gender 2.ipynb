{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking in tensors in PyTorch\n",
    "\n",
    "Hands-on training  by [Piotr Migdał](https://p.migdal.pl) (2019). Version 0.4 for Uniwersytet Śląski.\n",
    "\n",
    "**Work in progress**\n",
    "\n",
    "## Extra: LSTMs for on-hot encoded data\n",
    "\n",
    "We use recurrent networks. For wonderful introductions:\n",
    "\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah\n",
    "* [Exploring LSTMs](http://blog.echen.me/2017/05/30/exploring-lstms/) by Edwin Chen\t\n",
    "\n",
    "See also:\n",
    "\n",
    "* [Simple diagrams of convoluted neural networks](https://medium.com/inbrowserai/simple-diagrams-of-convoluted-neural-networks-39c097d2925b) by Piotr Migdał\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Kaprathy\n",
    "* [Repository to track the progress in Natural Language Processing](https://github.com/sebastianruder/NLP-progress) by Sebastian Ruder\n",
    "\n",
    "And a few technical remarks:\n",
    "\n",
    "* [Inconsistent dimension ordering for 1D networks - NCL vs NLC vs LNC](https://discuss.pytorch.org/t/inconsistent-dimension-ordering-for-1d-networks-ncl-vs-nlc-vs-lnc/14807)\n",
    "* [Contiguous() and permute()](https://discuss.pytorch.org/t/contiguous-and-permute/20673)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"../data/preprocessed/names.h5\", 'r') as data:\n",
    "    X_train = data['X_train'].value\n",
    "    y_train = data['y_train'].value\n",
    "    X_test = data['X_test'].value\n",
    "    y_test = data['y_test'].value\n",
    "    categories = [x.decode('utf-8') for x in data['categories'].value]\n",
    "    characters = [x.decode('utf-8') for x in data['characters'].value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.transpose((0, 2, 1)).copy()\n",
    "#X_test = X_test.transpose((0, 2, 1)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)  # should be: (24783, 26, 15)\n",
    "print(y_train.shape)  # should be: (24783)\n",
    "print(X_test.shape)   # should be: (8261, 26, 15)\n",
    "print(y_test.shape)   # should be: (8261)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.LongTensor(y_train)),\n",
    "                         batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(TensorDataset(torch.from_numpy(X_test), torch.LongTensor(y_test)),\n",
    "                         batch_size=32, shuffle=False)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": trainloader,\n",
    "    \"validation\": testloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on cuda if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    liveloss = PlotLosses()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.detach() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            prefix = ''\n",
    "            if phase == 'validation':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        \n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of networks in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(16 * 26, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(26, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(8 * 32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConvolutional(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(26, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        ) \n",
    "        \n",
    "        self.fc = nn.Linear(128, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.max(dim=2)[0]  # global max pool\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recurrent(nn.Module):\n",
    "    def __init__(self, rnn_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=26, hidden_size=rnn_size)\n",
    "        self.fc = nn.Linear(rnn_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        res = self.fc(cell).squeeze(0)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network creation and cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.from_numpy(X_train[:4])).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
