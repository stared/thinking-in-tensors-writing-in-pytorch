{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking in tensors, writing in PyTorch\n",
    "\n",
    "Hands-on training by [Piotr Migdał](https://p.migdal.pl), part of [Thinking in tensors, writing in PyTorch](https://github.com/stared/thinking-in-tensors-writing-in-pytorch).\n",
    "This notebook was supported by New Trends in Machine Learning by The University of Silesia in Katowice (2019).\n",
    "\n",
    "\n",
    "## Extra: Using an ImageNet-pretrained model\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/stared/thinking-in-tensors-writing-in-pytorch/blob/master/extra/Using%20an%20ImageNet-pretrained%20model.ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"/>\n",
    "</a>\n",
    "\n",
    "Do you want to use deep learning (so called \"AI\") to detect cats and dogs in a picture?\n",
    "Well, you can use one of many pre-trained ImageNet networks!\n",
    "\n",
    "I see there are many tutorials on:\n",
    "\n",
    "* training convolutional neural networks from scratch,\n",
    "* using a pre-trained neural network to detect new objects.\n",
    "\n",
    "But let's do something simpler - using a ready network. No training or tweaking needed!\n",
    "Before we go, let's play with [some browser-based demos](http://p.migdal.pl/interactive-machine-learning-list/), in this case [SqueezeNet v1.1 in Keras.js](https://transcranial.github.io/keras-js/#/squeezenet-v1.1) (depicted below) or [in ONNX.js](https://microsoft.github.io/onnxjs-demo/#/squeezenet).\n",
    "\n",
    "[![](imgs/squeezenet_fox_kerasjs.png)](https://transcranial.github.io/keras-js/#/squeezenet-v1.1)\n",
    "\n",
    "\n",
    "What is **ImageNet**, anyway?\n",
    "\n",
    "> The classification task is made up of 1.2 million images in the training set, each labeled with one of 1000 categories that cover a wide variety of objects, animals, scenes, and even some abstract geometric concepts such as “hook”, or “spiral”. - [What I learned from competing against a ConvNet on ImageNet](http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/) by Andrej Karpathy (2014) \n",
    "\n",
    "### Further references\n",
    "\n",
    "* [ImageNet hierarchy diagram](https://observablehq.com/@mbostock/imagenet-hierarchy) by Mike Bostock (I created [a bit racy version of that (NSFW!)](https://observablehq.com/@stared/tree-of-reddit-sex-life))\n",
    "* [ImageNet Neural Network Architectures](https://towardsdatascience.com/neural-network-architectures-156e5bad51ba) by Eugenio Culurciello (with their performance and sizes)\n",
    "* [Measuring the Progress of AI Research](https://www.eff.org/ai/metrics) by Electronic Frontier Foundation\n",
    "* [State of the Art - ImageNet Image Classification](https://paperswithcode.com/sota/image-classification-on-imagenet) by Papers with Code\n",
    "* [ImageNet API](http://image-net.org/download-API) (hierarchy, examples, etc)\n",
    " \n",
    "### Outline\n",
    "\n",
    "In this notebook we show how to (using PyTorch):\n",
    "\n",
    "* Load a pre-trained ImageNet model\n",
    "* Load a picture\n",
    "* Pass a picture trough a neural network to make the predictions...\n",
    "* ...and make sense of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "from IPython import display\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pre-trained model\n",
    "\n",
    "We can look from scripts scattered over different GitHub repositories. Fortunately, for ImageNet some models are built-in PyTorch.\n",
    "\n",
    "* OLD: [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html) (AlexNet, VGG, ResNet, SqueezeNet, DenseNet, Inception v3, GoogLeNet, ShuffleNet v2, MobileNet v2, ResNeXt)\n",
    "* NEW: [PyTorch Hub](https://pytorch.org/hub), also with other models, including [Natural Language Processing with GPT-2](https://pytorch.org/hub/huggingface_pytorch-pretrained-bert_gpt2/); see blog post [Towards Reproducible Research with PyTorch Hub](towards-reproducible-research-with-pytorch-hub/) (10 June 2019)\n",
    "\n",
    "Is is important that we: \n",
    "\n",
    "* load it with pretrained weights with `pretrained=True` (as opposed to only their architecture); note that some models are heavy (VGG16 weights approximately 500MB)\n",
    "* set it to evaluate mode with `.eval()` (some layers such as dropout or batch normalization work differently for training and evaluation).\n",
    "* move it to GPU `.to('cuda:0')` (but only if we have a CUDA-enabled GPU)\n",
    "\n",
    "With the last one we shouldn't be worried. While there is a significant speedup for using GPU, for prediction we should be fine. We use SqueezeNet v1.1 as it is small and fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the model with pretrained weights and set in in the eval mode\n",
    "model = models.squeezenet1_1(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have GPU, let's use that!\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading images\n",
    "\n",
    "Loading images is a simple step, once you know the basics (e.g. that PyTorch uses [Pillow (PIL)]((https://pillow.readthedocs.io/)) to preprocess images).\n",
    "\n",
    "* use PIL to load an image \n",
    "* use `transforms` to prepare it as a suitable tensor:\n",
    "    * resize and crop to match size with a network input (here: 224 x 224) \n",
    "    * scale colors\n",
    "* stack it into a 4-dimensional tensor (`batch x channels x width x height`)\n",
    "\n",
    "Even if we use a single image we need to use a batch (sample) of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path = \"../imgs/dog.jpeg\"\n",
    "img_path = \"https://raw.githubusercontent.com/stared/thinking-in-tensors-writing-in-pytorch/master/imgs/dog.jpeg\"\n",
    "\n",
    "if \":\" in img_path:\n",
    "    response = requests.get(img_path)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "else:\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to transform data for these models\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network sees\n",
    "transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop((224,224))\n",
    "])(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 image x 3 channels (RGB) x 224 x 224 pixels\n",
    "img_tensors = transform(img).unsqueeze(dim=0).to(device)\n",
    "img_tensors.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using model and making sense of it\n",
    "\n",
    "Finally, we can use a model!\n",
    "\n",
    "* it works with 4 dimensional tensors\n",
    "* if we need to pass data preprocessed (if we miss that it will perform worse, due to effectively seeing different colors, contracts or scale)\n",
    "\n",
    "And for all that we are rewarded with:\n",
    "\n",
    "* 1000 numbers per input image, no labels\n",
    "* ...and not even probabilities, but logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing an image with the model \n",
    "pred_logits_tensor = model(img_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get many numbers\n",
    "print(pred_logits_tensor.size())\n",
    "pred_logits_tensor[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to turn them into probabilities we need to perform softmax\n",
    "# then to use as a NumPy array we need to transfer it to CPU and convert\n",
    "pred_probs = pred_logits_tensor.softmax(dim=1).cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, but what do these number mean? Well, without any legend we can guess.\n",
    "\n",
    "* http://www.image-net.org/synset?wnid=n01440764\n",
    "* http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n04154340 for URLs to images with a given class\n",
    "\n",
    "Fortunately, there is an [imagenet_class_index.json](https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json) that I preprocessed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locally can be \"../data/imagenet_classes.csv\"\n",
    "imagenet_classes = pd.read_csv(\"https://raw.githubusercontent.com/stared/thinking-in-tensors-writing-in-pytorch/master/data/imagenet_classes.csv\", index_col='id')\n",
    "imagenet_classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add our predictions and show the most probable classes\n",
    "imagenet_classes['prediction'] = pred_probs[0]\n",
    "\n",
    "imagenet_classes.sort_values(by='prediction', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions_visually(img, imagenet_classes_with_preds):\n",
    "\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1,ncols=2, figsize=(7, 4))\n",
    "\n",
    "    ax0.imshow(img)\n",
    "\n",
    "    top_preds = imagenet_classes_with_preds.set_index('name')['prediction']\n",
    "    top_preds = top_preds.sort_values(ascending=False).head(10)\n",
    "    top_preds *= 100\n",
    "    top_preds.index.name = \"\"\n",
    "    top_preds.plot.bar(ax=ax1)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions_visually(img, imagenet_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it the corect class? let's check!\n",
    "r = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls\",\n",
    "                params={\"wnid\": \"n02106166\"})\n",
    "files = r.content.decode('utf8').split()\n",
    "\n",
    "filepath = files[0]\n",
    "print(filepath)\n",
    "display.Image(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's wrap it... in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(img_path,\n",
    "                     visually=True,\n",
    "                     imagenet_classes_path=\"https://raw.githubusercontent.com/stared/thinking-in-tensors-writing-in-pytorch/master/data/imagenet_classes.csv\"):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = models.squeezenet1_1(pretrained=True).eval().to(device)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if \":\" in img_path:\n",
    "        response = requests.get(img_path)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    else:\n",
    "        img = Image.open(img_path)\n",
    "    \n",
    "    img_tensor = transform(img).unsqueeze(dim=0).to(device)\n",
    "    \n",
    "    pred_logits_tensor = model(img_tensor)\n",
    "    pred_probs = pred_logits_tensor.softmax(dim=1).cpu().data.numpy()\n",
    "    \n",
    "    imagenet_classes = pd.read_csv(imagenet_classes_path, index_col='id')\n",
    "    imagenet_classes['prediction'] = pred_probs[0]\n",
    "    \n",
    "    if visually:\n",
    "        return show_predictions_visually(img, imagenet_classes)\n",
    "    else:\n",
    "        return imagenet_classes.sort_values(by='prediction', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locally can be \"../imgs/dog.jpeg\"\n",
    "show_predictions(\"https://raw.githubusercontent.com/stared/thinking-in-tensors-writing-in-pytorch/master/imgs/dog.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(\"http://farm1.static.flickr.com/106/284682545_454d85f1b2.jpg\",\n",
    "                 visually=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(\"https://live.staticflickr.com/8101/8557163376_ca33f48840_b.jpg\",\n",
    "                 visually=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, what's next?\n",
    "\n",
    "\n",
    "### Playing with it\n",
    "\n",
    "What would happen if you:\n",
    "\n",
    "* use different images?\n",
    "* use different ImageNet networks?\n",
    "* use different resize procedure (e.g. only `transforms.Resize((224, 224))`)?\n",
    "* execution time of a model depending if it uses CPU or GPU\n",
    "\n",
    "\n",
    "### Let's break things!\n",
    "\n",
    "What would happen if you:\n",
    "\n",
    "* remove `transforms.Normalize`?\n",
    "* set `pretrained=False`?\n",
    "\n",
    "\n",
    "### Next tutorials\n",
    "\n",
    "* Transfer learning - use a network to detect own classes!\n",
    "* Data augumentation - learn how to pre-process data\n",
    "\n",
    "(Both are Work in Progress in https://github.com/stared/thinking-in-tensors-writing-in-pytorch/tree/master/extra)\n",
    "\n",
    "\n",
    "### Footnote\n",
    "\n",
    "Brought to you by [Thinking in tensors, writing in PyTorch](https://github.com/stared/thinking-in-tensors-writing-in-pytorch) by Piotr Migdał. Follow me [@pmigdal](https://twitter.com/pmigdal).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
