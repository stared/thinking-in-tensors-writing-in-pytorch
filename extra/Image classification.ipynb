{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thinking in tensors in PyTorch\n",
    "\n",
    "Deep learning for neuroscientists - hands-on training  by [Piotr Migdał](https://p.migdal.pl) (2019). Version 0.2.\n",
    "\n",
    "\n",
    "## Extra: Image classification\n",
    "\n",
    "Notebook by Piotr Migdał, with some help from [Katarzyna Kańska](https://github.com/kkanska).\n",
    "\n",
    "Open in Colab: https://colab.research.google.com/github/stared/thinking-in-tensors-writing-in-pytorch/blob/master/extra/Image%20classification.ipynb\n",
    "\n",
    "Based on a beautiful dataset [Google Quickdraw](https://quickdraw.withgoogle.com/data), see also:\n",
    "\n",
    "* [Machine Learning for Visualization - Let’s Explore the Cutest Big Dataset](https://medium.com/@enjalot/machine-learning-for-visualization-927a9dff1cab) - Ian Johnson\n",
    "* [Train a model in tf.keras with Colab, and run it in the browser with TensorFlow.js](https://medium.com/tensorflow/train-on-google-colab-and-run-on-the-browser-a-case-study-8a45f9b1474e) - Zaid Alyafeai\n",
    "\n",
    "* Download \"apple\", \"brain\", \"octopus\", \"snowflake\" to `data`\n",
    "* `pip install livelossplot` - [Live training loss plot in Jupyter Notebook for Keras, PyTorch and others](https://github.com/stared/livelossplot/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run from colab\n",
    "!pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn.apionly as sns\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import urllib.request\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"cat\", \"dog\", \"spider\", \"octopus\", \"snowflake\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download classes in necessary\n",
    "base_url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
    "for c in classes:\n",
    "    path = '{}{}.npy'.format(base_url, c.replace('_', '%20'))\n",
    "    print(path)\n",
    "    urllib.request.urlretrieve(path, \"data/{}.npy\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's inside?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!ls data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data loading\n",
    "\n",
    "I.e. the boring part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "size = 28\n",
    "limit = 500\n",
    "\n",
    "X_list = []\n",
    "\n",
    "for c in classes:\n",
    "    X_c = np.load(\"data/{}.npy\".format(c))  # or \"../data/full_numpy_bitmap_{}.npy\"\n",
    "    print(\"Loaded {} out of {} {}s\".format(limit, X_c.shape[0], c))\n",
    "    X_list.append(X_c[:limit])\n",
    "\n",
    "X = np.concatenate(X_list)\n",
    "Y = np.concatenate([limit * [i] for i in range(len(classes))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].reshape(28, 28)[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = X.reshape(-1, 1, size, size)\n",
    "X = X.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# (samples, channels, x, y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# answer keys are integers\n",
    "Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First, let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train[53].reshape(28, 28), cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def draw_examples(X, Y, classes, rows=6, scale=1):\n",
    "    fig, axs = plt.subplots(rows, len(classes), figsize=(scale * len(classes), scale * rows))\n",
    "    size = X.shape[-1]\n",
    "    for class_id in range(len(classes)):\n",
    "        X_class = X[Y == class_id]\n",
    "        for i in range(rows):\n",
    "            ax = axs[i, class_id]\n",
    "            x = X_class[np.random.randint(len(X_class))].reshape(size, size)\n",
    "            ax.imshow(x, cmap='Greys', interpolation='none')\n",
    "            ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "draw_examples(X_train, Y_train, classes, rows=6, scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Per-class averages\n",
    "\n",
    "Vide [this tweet](https://twitter.com/kcimc/status/902229612666658816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def draw_class_averages(X, Y, classes, scale=2):\n",
    "    fig, axs = plt.subplots(1, len(classes), figsize=(scale * len(classes), scale))\n",
    "    size = X.shape[-1]\n",
    "    for class_id in range(len(classes)):\n",
    "        X_class = X[Y == class_id]\n",
    "        ax = axs[class_id]\n",
    "        x = X_class.mean(axis=0).reshape(size, size)\n",
    "        ax.imshow(x, cmap='Greys', interpolation='none')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "draw_class_averages(X_train, Y_train, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and data loaders\n",
    "\n",
    "We need to create data loaders to load and preprocess data. We use split:\n",
    "* train - for training,\n",
    "* validation - not used for training, but to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(Y_train).long().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download CIFAR10 train and validation datasets\n",
    "\n",
    "# define data loaders\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(Y_train).long()),\n",
    "               batch_size=64,\n",
    "               shuffle=True, num_workers=4),\n",
    "    'validation': \n",
    "    DataLoader(TensorDataset(torch.from_numpy(X_test), torch.from_numpy(Y_test).long()),\n",
    "               batch_size=64,\n",
    "               shuffle=False, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start\n",
    "\n",
    "While training a model, it is important to set `train` or `eval` mode of the model, as some layers have different behavior during train and evaluation.\n",
    "\n",
    "See also: [Keras vs. PyTorch: Alien vs. Predator recognition with transfer learning](https://deepsense.ai/keras-vs-pytorch-avp-transfer-learning) which explains API differences between these frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    liveloss = PlotLosses()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += (preds == labels.data).sum().item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            prefix = ''\n",
    "            if phase == 'validation':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss\n",
    "            logs[prefix + 'accuracy'] = epoch_acc\n",
    "        \n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "Multi-class logistic regression can be expressed as a shallow neural network consisting of one linear layer and a softmax activation function.\n",
    "\n",
    "For binary classification, we can use sigmoid (a.k.a. logistic function):\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)} $$\n",
    "\n",
    "Softmax function transforms any vector into distribution vector (values in range (0., 1.) that sum up to 1.):\n",
    "$$\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$\n",
    "\n",
    "We use a cross-entropy loss function:\n",
    "$$- \\sum_j p_{j, true} \\log(p_{j, pred})$$\n",
    "\n",
    "Note that we do not state explicitly the softmax function in the model class below. For details see [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss).\n",
    "\n",
    "See also:\n",
    "\n",
    "* [Cross-entropy vs. mean-squared error loss](https://www.reddit.com/r/MachineLearning/comments/8im9eb/d_crossentropy_vs_meansquared_error_loss/)\n",
    "* [Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n",
    "* [Cross entropy](https://pandeykartikey.github.io/machine/learning/basics/2018/05/22/cross-entropy.html) - another explanation\n",
    "* [Softmax function](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "* [Multiclass logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28 * 28, len(classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=100.)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model_trained = train_model(model, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_logit = model(torch.from_numpy(X_test[:5]))\n",
    "pred_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(pred_logit, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Make some changes and see how it goes.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Test optim.SGD learning rate (e.g. x0.1 and x10).\n",
    "* Use optim.Adam instead of optim.SGD.\n",
    "\n",
    "Optimizers are important, see:\n",
    "\n",
    "* [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/) by Sebastian Ruder\n",
    "* [SGD > Adam?? Which One Is The Best Optimizer: Dogs-VS-Cats Toy Experiment | SALu](https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/)\n",
    "\n",
    "tl;dr: If you don't now what to do, use Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Old school neural network\n",
    "\n",
    "Linear layers are also called dense layers or fully-connected layers. Stacking a few of them gives a model called multilayer perceptron (MLP). Importantly, we need to use an activation function for our network to be nonlinear transformation. Here we use sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_1=128, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        func = {'sigmoid': nn.Sigmoid(), \n",
    "                'relu': nn.ReLU(),\n",
    "                'tanh': nn.Tanh()}[activation]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1 * 28 * 28, hidden_1),\n",
    "            func,\n",
    "            #nn.Linear(hidden_1, hidden_1),\n",
    "            #func,\n",
    "            nn.Linear(hidden_1, len(classes))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = MLP(hidden_1=2048)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model_trained = train_model(model, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Make some changes and see how it goes.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Use Tanh or ReLU instead of Sigmoid.\n",
    "* Use more than 20 epochs.\n",
    "* In practice, neural networks use 2-3 dense layers.\n",
    "* Make big changes to see a difference. In this case change the hidden layer size by 2x or even 10x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional neural network\n",
    "\n",
    "Treating an image as a flat vector looses its spatial structure. Instead we can use the spacial structure in our advantage and perform convolutions.\n",
    "Convolution is an operation which performs the same local operation on each part of the image.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/blob/master/gif/same_padding_no_strides.gif?raw=true)\n",
    "\n",
    "Each convolution layer produces new channels based on those which preceded it. First, we start with 3 channels for red, green and blue (RGB) components. Next, channels get more and more abstract.\n",
    "\n",
    "While producing new channels with representations of various properties of the image, we also reduce the resolution, usually using pooling layers.\n",
    "\n",
    "See also:\n",
    "* [Image Kernels - visually explained](http://setosa.io/ev/image-kernels/)\n",
    "* [How neural networks build up their understanding of images](https://distill.pub/2017/feature-visualization/)\n",
    "* source of above image: [Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic)\n",
    "* [Convolutional Neural Networks by Andrej Karpathy](http://cs231n.github.io/convolutional-networks/) for in-depth explanation of convolutions and other accompanying blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Linear(32 * 7 * 7, len(classes))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# or we can modularize that\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            self._block(1, 16),\n",
    "            self._block(16, 32)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 7 * 7, len(classes)) # dropout pomiedzy gestymi\n",
    "        )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(),  # batch norm 2d\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model_trained = train_model(model, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now, feel free to experiment.\n",
    "\n",
    "Hints:\n",
    "* Play with the number of channels and how they grow.\n",
    "* Usually 3×3 convolutions work the best; stick to them (and 1×1 convolutions which only mix channels).\n",
    "* You can have 1-3 convolutional layers before each MaxPool operation.\n",
    "* Adding a Dense layer may help.\n",
    "* Between dense layers you can use Dropout, to reduce overfitting (i.e. if you see that training accuracy is higher than validation accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_preditions(model, X, Y, classes, rows=6, only_wrong=False):\n",
    "    \n",
    "    # very greedy\n",
    "    preds = F.softmax(model(torch.from_numpy(X)), dim=1).detach().numpy()\n",
    "    \n",
    "    if only_wrong:\n",
    "        incorrect = preds.argmax(1) != Y\n",
    "        preds = preds[incorrect]\n",
    "        X = X[incorrect]\n",
    "        Y = Y[incorrect]\n",
    "\n",
    "    fig, axs = plt.subplots(rows, 2, figsize=(8, 1.5 * rows))\n",
    "    for i in range(rows):\n",
    "        ax = axs[i, 0]\n",
    "        ax.imshow(X[i].reshape(size, size),\n",
    "                  cmap='Greys', interpolation='none')\n",
    "\n",
    "        ax.axis('off')\n",
    "    \n",
    "        pd.DataFrame({\"pred\": preds[i], \"true\": [int(Y[i] == j) for j in range(len(classes))]}, index=classes) \\\n",
    "          .plot(kind='barh', ax=axs[i, 1], xlim=[0, 1], stacked=True, legend=False)\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_preditions(model, X_train, Y_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_preditions(model, X_test, Y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_preditions(model, X_test, Y_test, classes, only_wrong=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantify confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(torch.from_numpy(X_test)).detach().numpy().argmax(1)\n",
    "cm = confusion_matrix(Y_test, preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "cm_df.columns.name = \"predicted\"\n",
    "cm_df.index.name = \"ground truth\"\n",
    "\n",
    "plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_image_matrix(model, X, Y, classes, size=28):\n",
    "    confused = np.zeros((len(classes), len(classes), size, size), dtype='float32')\n",
    "    Y_pred = model(torch.from_numpy(X)).detach().numpy().argmax(1)\n",
    "    for x, y_true, y_pred in zip(X, Y, Y_pred):\n",
    "        confused[y_true, y_pred] = x[0, :, :]\n",
    "\n",
    "    fig, axs = plt.subplots(len(classes), len(classes), figsize=(2*len(classes), 2*len(classes)))\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ax = axs[i, j]\n",
    "            ax.imshow(confused[i, j], cmap='Greys', interpolation='none')\n",
    "            ax.axis('off')\n",
    "\n",
    "    fig.suptitle('predicted', fontsize=16)\n",
    "    for ax, c in zip(axs[0], classes):\n",
    "            ax.set_title(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "confusion_image_matrix(model, X_test, Y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further notes\n",
    "\n",
    "If you want to learn more, some relevant blog posts:\n",
    "\n",
    "* [Data science intro for math/phys background](http://p.migdal.pl/2016/03/15/data-science-intro-for-math-phys-background.html)\n",
    "* [Learning Deep Learning with Keras](https://p.migdal.pl/2017/04/30/teaching-deep-learning.html)\n",
    "* [Keras or PyTorch as your first deep learning framework](https://deepsense.ai/keras-or-pytorch/) (previously with an inflammatory title *Don't learn TensorFlow - start with Keras or PyTorch instead*)\n",
    "* [Keras vs. PyTorch: Alien vs. Predator recognition with transfer learning](https://deepsense.ai/keras-vs-pytorch-avp-transfer-learning/) with interactive code in Jupyter Notebook: https://www.kaggle.com/pmigdal/alien-vs-predator-images/kernels\n",
    "* [Simple diagrams of convoluted neural networks](https://medium.com/inbrowserai/simple-diagrams-of-convoluted-neural-networks-39c097d2925b) - In Browser AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
